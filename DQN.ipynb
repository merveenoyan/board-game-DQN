{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "debug_RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow import GradientTape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import initializers\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "AQybbE4Iona3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game_runner = GameRunner(agent_1 = agent_1, agent_2 = agent_2, model = q_network, pieces = pieces, board = board)"
      ],
      "metadata": {
        "id": "EYZ42ld-TEZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game_runner.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "koSMa4ecThid",
        "outputId": "253fe7e8-2778-4e6e-b358-12dc1ae9ad26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actions is []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-6eb5e53129bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-164-8665d776e3d0>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-175-320a695c0ddd>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state, step, eps)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m#if random.random() < eps:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRB9BeYAmnQ_"
      },
      "outputs": [],
      "source": [
        "class GameRunner:\n",
        "  \n",
        "  def __init__(self, agent_1, agent_2, model, pieces, board):\n",
        "    \n",
        "    self.agent_1 = agent_1\n",
        "    self.agent_2 = agent_2\n",
        "    self.model = model\n",
        "    self.pieces = pieces\n",
        "    self.steps = 500\n",
        "    self.board = board\n",
        "    #self.state = board\n",
        "    self.next_state = None\n",
        "    self.min_eps = 0.01\n",
        "    self.eps = 0.01\n",
        "    self.max_eps = 1\n",
        "    self.step = None\n",
        "    \n",
        "  def run(self):\n",
        "      \n",
        "      epsilon = 0.4\n",
        "      lambd = 0.0001\n",
        "      gamma = 0.99\n",
        "      batch_size = 50\n",
        "      learning_rate = 0.01\n",
        "      \n",
        "\n",
        "      # Define optimizer\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "      for current_step in range(self.steps):\n",
        "\n",
        "          self.step = current_step\n",
        "          if self.step % 2 == 0:\n",
        "            agent = self.agent_1\n",
        "\n",
        "          else:\n",
        "            agent = self.agent_2\n",
        "\n",
        "\n",
        "          if agent == self.agent_1:\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                action = self.agent_1.choose_action(self.board, self.step, self.eps)\n",
        "\n",
        "                q_values = q_network(self.board.flatten())\n",
        "                \n",
        "                epsilon = np.random.rand()\n",
        "\n",
        "                # choose the action\n",
        "\n",
        "                actions = self.agent_1.get_available_actions(self.board, self.pieces)\n",
        "        \n",
        "                if steps < 100:\n",
        "                    action = random.choice(actions)\n",
        "                elif steps > 100:\n",
        "                    if random.random() < self.eps:\n",
        "                        action = random.choice(actions)\n",
        "                    else:\n",
        "                        action = np.argmax(self.model.predict(self.board.flatten()))\n",
        "\n",
        "                self.eps = self.min_eps + (self.max_eps - self.min_eps) * math.exp(-lambd * self.steps) # update epsilon\n",
        "\n",
        "                reward = calculate_reward(action)\n",
        "\n",
        "                q_value = q_values[0, action]\n",
        "                loss_value = mean_squared_error_loss(q_value, reward)\n",
        "                grads = tape.gradient(loss_value[0], q_network.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
        "            \n",
        "          else:\n",
        "            actions = self.agent_2.get_available_actions(self.board, self.pieces, action)\n",
        "\n",
        "\n",
        "            action = self.agent_2.choose_action(self.board, self.step, self.eps)\n",
        "\n",
        "            for act in actions:\n",
        "                \n",
        "                action_dict = {}\n",
        "                action_dict[act] = self.calculate_reward(act, pieces)\n",
        "                action = max(action_dict, key=action_dict.get)\n",
        "\n",
        "          next_state, reward, done = self.update_state(agent, pieces, board)\n",
        "\n",
        "          if done:\n",
        "              next_state = None \n",
        "          \n",
        "          agent_1.memory.add_sample((self.board, action, reward, next_state))\n",
        "          agent_1.replay()\n",
        "\n",
        "\n",
        "          board = next_state \n",
        "          \n",
        "          agent_1.total_reward += reward\n",
        "          \n",
        "          agent_2.total_reward += reward\n",
        "\n",
        "          if done:\n",
        "              self.reward_store.append(total_reward)\n",
        "\n",
        "          print(f\"Step {self.step}, Total reward: {total_reward}, Episodes: {episodes}\")\n",
        "\n",
        "\n",
        "  # experience replay for DQN\n",
        "  def replay(self):\n",
        "    batch = self.memory.sample(self.model.batch_size) # take random batch from memory\n",
        "\n",
        "    states = np.array([val[0] for val in batch])\n",
        "\n",
        "    next_states = np.array([(np.zeros(self.model.num_states)\n",
        "                              if val[3] is None else val[3]) for val in batch])\n",
        "\n",
        "    # predict Q(s,a) given the batch of states\n",
        "    q_s_a = self.model.predict_on_batch(states)\n",
        "\n",
        "    q_s_a_d = self.model.predict_on_batch(next_states)\n",
        "\n",
        "    # setup training arrays\n",
        "    x = states\n",
        "    y = np.zeros((len(batch), 196))\n",
        "\n",
        "    for i, b in enumerate(batch):\n",
        "        state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
        "        \n",
        "        current_q = q_s_a[i] # current Q value\n",
        "        \n",
        "        if next_state is None:\n",
        "            # when game is done\n",
        "            current_q[action] = reward\n",
        "        else: # update Q values\n",
        "            current_q[action] = reward + gamma * np.amax(q_s_a_d[i])\n",
        "        \n",
        "        x[i] = state\n",
        "        y[i] = current_q\n",
        "      \n",
        "\n",
        "  def update_state(self, agent):\n",
        "\n",
        "      done = False\n",
        "\n",
        "      for piece in self.pieces:\n",
        "\n",
        "          if piece.player == 0:\n",
        "              player_pieces.append(piece)\n",
        "          else:\n",
        "              opponent_pieces.append(piece)\n",
        "      \n",
        "      if player_pieces == [] or opponent_pieces == []:\n",
        "          done = True\n",
        "\n",
        "      board = agent.move(board, action)\n",
        "\n",
        "      reward = agent.calculate_reward(action, self.pieces)\n",
        "\n",
        "      next_state = self.get_next_state(self.pieces)\n",
        "\n",
        "      return next_state, reward, done \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent: \n",
        "  \n",
        "  def __init__(self, player, model = None, memory = None):\n",
        "\n",
        "    self.model = model\n",
        "    self.memory = memory\n",
        "    self.player = player # can be 1 or 2, used to match agents with their pieces\n",
        "\n",
        "\n",
        "\n",
        "    # get the list available actions based on positioning of the pieces\n",
        "    # each action is a list [increment in x, increment in y, piece itself]\n",
        "\n",
        "  def get_available_actions(self, board, pieces):\n",
        "        \n",
        "    available_actions = []\n",
        "    player_pieces, opponent_pieces = self.get_pieces(pieces)\n",
        "\n",
        "    for piece in player_pieces:\n",
        "        \n",
        "        if piece.is_on_end == False:\n",
        "\n",
        "            if board[piece.x + 1, piece.y] == 0:\n",
        "                available_actions.append((1, 0, piece.x, piece.y))\n",
        "\n",
        "            elif board[piece.x - 1, piece.y] == 0:\n",
        "                available_actions.append((-1, 0, piece.x, piece.y))\n",
        "\n",
        "            elif board[piece.x, piece.y - 1] == 0:\n",
        "                available_actions.append((0, -1, piece.x, piece.y))\n",
        "\n",
        "            if board[piece.x, piece.y+1] == 0:\n",
        "                available_actions.append((0, 1, piece.x, piece.y))\n",
        "            \n",
        "        if piece.is_on_end == True:\n",
        "\n",
        "            if piece.x == 6: # bottom right corner\n",
        "                if piece.y == 6:\n",
        "                    if board[ piece.x - 1, piece.y ] == 0: # check if the position next to piece is not occupied\n",
        "                        available_actions.append([-1, 0, piece])\n",
        "                    if board[ piece.x, piece.y - 1] == 0:\n",
        "                        available_actions.append([0, -1, piece])\n",
        "                \n",
        "                elif piece.y == 0:  \n",
        "                    if board[ piece.x - 1, piece.y ] == 0:\n",
        "                        available_actions.append([-1, 0, piece])\n",
        "                    \n",
        "                    if board[ piece.x, piece.y + 1] == 0:\n",
        "                        available_actions.append([0, 1, piece])\n",
        "                else:\n",
        "                    if board[ piece.x - 1, piece.y ] == 0:\n",
        "                        available_actions.append([-1, 0, piece])\n",
        "                    if board[ piece.x, piece.y + 1] == 0:\n",
        "                        available_actions.append([0, 1, piece])\n",
        "                    if board[ piece.x + 1, piece.y ] == 0:\n",
        "                        available_actions.append([1, 0, piece])\n",
        "\n",
        "            if piece.x == 0: # top left corner\n",
        "                if piece.y == 0: \n",
        "                    if board[ piece.x + 1, piece.y] == 0:\n",
        "                        available_actions.append([1, 0, piece])\n",
        "                    if board[ piece.x, piece.y + 1] == 0:\n",
        "                        available_actions.append([0, 1, piece])\n",
        "                elif piece.y == 6:\n",
        "                    if board[ piece.x + 1, piece.y ] == 0:\n",
        "                        available_actions.append([1, 0, piece])\n",
        "                    if board[ piece.x, piece.y - 1] == 0:\n",
        "                        available_actions.append([0, -1, piece])\n",
        "                else:\n",
        "                    if board[ piece.x + 1, piece.y ] == 0:\n",
        "                        available_actions.append([1, 0, piece])\n",
        "                    if board[ piece.x, piece.y + 1] == 0:\n",
        "                        available_actions.append([0, 1, piece])\n",
        "                    if board[ piece.x, piece.y - 1] == 0:\n",
        "                        available_actions.append([0, -1, piece])\n",
        "        \n",
        "    return available_actions\n",
        "\n",
        "\n",
        "  # get reward based on pieces or termination\n",
        "  def calculate_reward(self, action, pieces):\n",
        "        \n",
        "    if self.player == 1:\n",
        "        opponent = agent_2\n",
        "    else: \n",
        "        opponent = agent_1\n",
        "    player_pieces, opponent_pieces = self.get_pieces(pieces)\n",
        "\n",
        "    # | X O \n",
        "    # if piece is on edge\n",
        "\n",
        "    for player_piece in player_pieces:\n",
        "        for opponent_piece in opponent_pieces:\n",
        "            \n",
        "            #defensive\n",
        "\n",
        "            if player_piece.y == 0 and opponent_piece.y == 0: \n",
        "                if player_piece.x == opponent_piece.x - 1:\n",
        "                    \n",
        "                    reward = -5\n",
        "                    #player_piece.discarded = 1 \n",
        "                    # board[player_piece.x, player_piece.y] = 0\n",
        "                    # player_pieces.pop(player_piece)\n",
        "\n",
        "            elif player_piece.y == 6 and opponent_piece.y == 6: \n",
        "                if player_piece.x == opponent_piece.x + 1:\n",
        "                    reward = -5\n",
        "                    \n",
        "            elif player_piece.x == 6 and opponent_piece.x == 6: \n",
        "\n",
        "                if player_piece.y == opponent_piece.y + 1:\n",
        "                    reward = -5\n",
        "\n",
        "            elif player_piece.x == 0 and opponent_piece.x == 0: \n",
        "\n",
        "                if player_piece.y == opponent_piece.y - 1:\n",
        "                    reward = -5\n",
        "\n",
        "            # aggressive\n",
        "            if player_piece.y == 0 and opponent_piece.y == 0:\n",
        "                if player_piece.x == opponent_piece.x + 1:\n",
        "                    \n",
        "                    reward = 5\n",
        "                    # player_piece.discarded = 1 \n",
        "                    # board[player_piece.x, player_piece.y] = 0\n",
        "                    # player_pieces.pop(player_piece)\n",
        "\n",
        "            elif player_piece.y == 6 and opponent_piece.y == 6: # O X |\n",
        "                if player_piece.x == opponent_piece.x - 1:\n",
        "                    reward = 5\n",
        "                    \n",
        "            elif player_piece.x == 6 and opponent_piece.x == 6: \n",
        "\n",
        "                if player_piece.y == opponent_piece.y - 1:\n",
        "                    reward = 5\n",
        "\n",
        "            elif player_piece.x == 0 and opponent_piece.x == 0: \n",
        "\n",
        "                if player_piece.y == opponent_piece.y + 1:\n",
        "                    reward = 5\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "    \n",
        "  # choose reward based action or q-value based action\n",
        "  def choose_action(self, state, step, eps):\n",
        "    #import pdb;pdb.set_trace()\n",
        "\n",
        "    actions = self.get_available_actions(board, pieces)\n",
        "    print(f\"actions is {actions}\")\n",
        "    \n",
        "    if self.player == 1:\n",
        "\n",
        "        if step < 100:\n",
        "            return random.choice(actions)\n",
        "        elif step > 100:\n",
        "            #if random.random() < eps:\n",
        "            #    return random.choice(actions)\n",
        "            #else:\n",
        "            return np.argmax(self.model.predict(state))\n",
        "    else:\n",
        "\n",
        "        for action in actions:\n",
        "            action_dict = {}\n",
        "            action_dict[action] = self.calculate_reward(action, pieces)\n",
        "            best_move = max(action_dict, key=action_dict.get)\n",
        "        \n",
        "        return best_move\n",
        "\n",
        "  def move(self, board, action, agent):\n",
        "        \n",
        "    action = self.choose_action(state = board, step = GameRunner.step, eps = 0.5)\n",
        "\n",
        "    piece = action[2]\n",
        "\n",
        "    old_pos = [piece.x, piece.y] # keep old position\n",
        "\n",
        "    board[piece.x + action[0], piece.y + action[1]] = piece.player # update board\n",
        "    \n",
        "    piece.x += action[0] #update piece\n",
        "    piece.y += action[1]\n",
        "    \n",
        "    pieces = pieces.remove(piece)\n",
        "\n",
        "    board[old_pos[0], old_pos[1]] = 0\n",
        "\n",
        "    return board\n",
        "\n",
        "\n",
        "  # get list of pieces for each player\n",
        "  def get_pieces(self, pieces):\n",
        "\n",
        "      player_pieces = []\n",
        "      opponent_pieces = []\n",
        "\n",
        "      for piece in pieces:\n",
        "        if self.player == piece.player:\n",
        "          player_pieces.append(piece)\n",
        "        else:\n",
        "          opponent_pieces.append(piece)\n",
        "\n",
        "      return player_pieces, opponent_pieces\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NPpG5elknqtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Piece:\n",
        "\n",
        "    def __init__(self, x, y, discarded, player):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.discarded = discarded\n",
        "        self.player = player\n",
        "\n",
        "    def is_on_end(self, x, y):\n",
        "        \n",
        "        if self.x == 6 or self.x == 0:\n",
        "            return True\n",
        "        if self.y == 6 or self.y == 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n"
      ],
      "metadata": {
        "id": "uh06jXH1ni67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def construct_q_network():\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(49))  # size of states raveled, board flattened\n",
        "    hidden1 = tf.keras.layers.Dense(\n",
        "        128, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
        "    )(inputs)\n",
        "    hidden2 = tf.keras.layers.Dense(\n",
        "        256, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
        "    )(hidden1)\n",
        "    hidden3 = tf.keras.layers.Dense(\n",
        "        192, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
        "    )(hidden2)\n",
        "    q_values = tf.keras.layers.Dense(\n",
        "        196, kernel_initializer=initializers.Zeros(), activation=\"softmax\"\n",
        "    )(hidden3) # number of actions is board.x * board.y * moves\n",
        "\n",
        "    deep_q_network = tf.keras.Model(inputs=inputs, outputs=[q_values])\n",
        "\n",
        "    return deep_q_network"
      ],
      "metadata": {
        "id": "qUjDGObtodmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_memory):\n",
        "        self.max_memory = max_memory\n",
        "        self.samples = []\n",
        "\n",
        "    def add_sample(self, sample):\n",
        "        self.samples.append(sample)\n",
        "        if len(self.samples) > self.max_memory:\n",
        "            self.samples.pop(0)\n",
        "\n",
        "    def sample(self, no_samples):\n",
        "        if no_samples > len(self._samples):\n",
        "            return random.sample(self.samples, len(self.samples))\n",
        "        else:\n",
        "            return random.sample(self.samples, no_samples)\n",
        "\n"
      ],
      "metadata": {
        "id": "2HTrTYa5ohe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_board():\n",
        "\n",
        "    board = np.zeros([7,7])\n",
        "\n",
        "    piece_1_1 = Piece(x = 0, y = 0, discarded = 0, player = 1)\n",
        "    piece_1_2 = Piece(x = 0, y = 2, discarded = 0, player = 1)\n",
        "    piece_1_3 = Piece(x = 0, y = 4, discarded = 0, player = 2)\n",
        "    piece_1_4 = Piece(x = 6, y = 6, discarded = 0, player = 1)\n",
        "\n",
        "    piece_2_1 = Piece(x = 6, y = 4, discarded = 0, player = 1)\n",
        "    piece_2_2 = Piece(x = 0, y = 6, discarded = 0, player = 2)\n",
        "    piece_2_3 = Piece(x = 6, y = 0, discarded = 0, player = 2)\n",
        "    piece_2_4 = Piece(x = 6, y = 2, discarded = 0, player = 2)\n",
        "\n",
        "    pieces = [piece_1_1, piece_1_2, piece_1_3, piece_1_4,\n",
        "              piece_2_1, piece_2_2, piece_2_3, piece_2_4]\n",
        "    for piece in pieces:\n",
        "\n",
        "      board[piece.x, piece.y] = piece.player\n",
        "\n",
        "    return pieces, board"
      ],
      "metadata": {
        "id": "nGxSkIuyqTe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pieces, board = init_board()"
      ],
      "metadata": {
        "id": "olGtrbQ6mrKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-weTp6opzi9w",
        "outputId": "cdf84f69-adba-43e7-c8a0-88e6e7d1d55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 1., 0., 2., 0., 2.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [2., 0., 2., 0., 1., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = Memory(max_memory = 100)"
      ],
      "metadata": {
        "id": "b7ej7RPW0s8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_network = construct_q_network()"
      ],
      "metadata": {
        "id": "1zLCuiz604F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_network.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmuUXY1Ltkx-",
        "outputId": "f1b80b8a-09af-4283-f5bc-77ceee23135b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 49)]              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 128)               6400      \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 192)               49344     \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 196)               37828     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 126,596\n",
            "Trainable params: 126,596\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_1 = Agent(player = 1, model = q_network, memory = memory)"
      ],
      "metadata": {
        "id": "uP9p0A_M03pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_2 = Agent(player = 2, model = None,  memory = None)"
      ],
      "metadata": {
        "id": "QAPcrjyO0upo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8RtdobiuREBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "    # choose reward based action or q-value based action\n",
        "    def choose_action(self, state, action):\n",
        "\n",
        "        actions = agent.get_available_actions(board, pieces, action)\n",
        "        \n",
        "        if self.player == 1:\n",
        "\n",
        "            if steps < 100:\n",
        "                return random.choice(actions)\n",
        "            elif steps > 100:\n",
        "                if random.random() < self.eps:\n",
        "                    return random.choice(actions)\n",
        "                else:\n",
        "                    return np.argmax(self.model.predict(state))\n",
        "        else:\n",
        "\n",
        "            for action in actions:\n",
        "                action_dict = {}\n",
        "                action_dict[action] = self.calculate_reward(action, pieces)\n",
        "                best_move = max(action_dict, key=action_dict.get)\n",
        "            \n",
        "            return best_move\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.state_dim = state_dim \n",
        "        self.action_dim = action_dim\n",
        "        self.q_values = q_values # Q-values of state, action, a tuple\n",
        "        self.num_states = num_states # number of potential board configurations\n",
        "        self.num_actions = num_actions # number of actions, 7*7*4\n",
        "        self.states = states\n",
        "        self.actions = None # moving a particular piece to up/down/left/right, a tuple\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    def add_state(self, states, pieces):\n",
        "\n",
        "      ## [[state 1], [[piece 1.x, piece 1.y, piece1.player, piece 1.discarded], [piece_2], ...]\n",
        "      \n",
        "      piece_states = []\n",
        "\n",
        "      for piece in self.pieces:\n",
        "\n",
        "        piece_state = [piece.x, piece.y, piece.player, piece.discarded]\n",
        "        piece_states.append(piece_state)\n",
        "      states.append(piece_states)\n",
        "\n",
        "      return states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_next_state(self, pieces):\n",
        "\n",
        "      next_state = []\n",
        "\n",
        "      for piece in self.pieces:\n",
        "\n",
        "        pieces_state = [piece.x, piece.y, piece.player, piece.discarded]\n",
        "        next_state.append(pieces_state)\n",
        "      \n",
        "\n",
        "      return next_state\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "R_fn7pICngu3",
        "outputId": "32d990f8-54e8-4b3c-9675-acd22e06a621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n    def __init__(self):\\n\\n        self.state_dim = state_dim \\n        self.action_dim = action_dim\\n        self.q_values = q_values # Q-values of state, action, a tuple\\n        self.num_states = num_states # number of potential board configurations\\n        self.num_actions = num_actions # number of actions, 7*7*4\\n        self.states = states\\n        self.actions = None # moving a particular piece to up/down/left/right, a tuple\\n    \\n\\n\\n\\n    def add_state(self, states, pieces):\\n\\n      ## [[state 1], [[piece 1.x, piece 1.y, piece1.player, piece 1.discarded], [piece_2], ...]\\n      \\n      piece_states = []\\n\\n      for piece in self.pieces:\\n\\n        piece_state = [piece.x, piece.y, piece.player, piece.discarded]\\n        piece_states.append(piece_state)\\n      states.append(piece_states)\\n\\n      return states\\n\\n\\n\\n\\n    def get_next_state(self, pieces):\\n\\n      next_state = []\\n\\n      for piece in self.pieces:\\n\\n        pieces_state = [piece.x, piece.y, piece.player, piece.discarded]\\n        next_state.append(pieces_state)\\n      \\n\\n      return next_state\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iQarYYTEqs1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}